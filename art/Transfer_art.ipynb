{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Dataset: https://tev.fbk.eu/technologies/smartwatch-gestures-dataset\n",
    "\n",
    "Research Paper: https://www.eurasip.org/Proceedings/Eusipco/Eusipco2014/HTML/papers/1569922319.pdf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " <h2>Import dataset to get started<h2>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read in from csv\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# df=pd.read_csv('gesture_data.csv')\n",
    "# print(\"data read in from csv\")\n",
    "#\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# samples = pd.unique(df['sample'])\n",
    "# iteration = pd.unique(df['iteration'])\n",
    "# gesture = pd.unique(df['gesture'])\n",
    "# user = pd.unique(df['user'])\n",
    "# dat=np.zeros((len(user),len(gesture),len(iteration),len(samples)+1,3))\n",
    "# for i in pd.unique(df['user']):\n",
    "#     for j in pd.unique(df['gesture']):\n",
    "#         for k in pd.unique(df['iteration']):\n",
    "#             d=df[(df['user'] == i) & (df['gesture'] == j) & (df['iteration'] == k)][['accel0','accel1','accel2']].to_numpy()\n",
    "#             d_sh = np.shape(d)\n",
    "#             dat[i,j,k,:d_sh[0]] = d\n",
    "#             dat[i,j,k,-1] = [i,j,k]\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# np.savez('gesture',dat)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data read in using numpy file\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "da=np.load('gesture.npz')\n",
    "data=da[da.files[0]]\n",
    "print(\"data read in using numpy file\")\n",
    "\n",
    "# #%% md\n",
    "#\n",
    "# <h2>Profile Data and divide into train test sets into <h2>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_sh = np.shape(data)\n",
    "val_ts_user = np.random.choice(range(data_sh[0]),2,replace=False)\n",
    "ts_user = val_ts_user[0]\n",
    "val_user = val_ts_user[1:]\n",
    "print('Users randomly selected for test isolation')\n",
    "print(ts_user)\n",
    "print('Users randomly selected for validation')\n",
    "print(ts_user)\n",
    "train_set = data.copy()\n",
    "test_set =  data[ts_user]\n",
    "val_set = data[val_user]\n",
    "train_set = np.delete(train_set,val_ts_user,axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% train test data splits\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users randomly selected for test isolation\n",
      "[0 7]\n"
     ]
    }
   ],
   "source": [
    "n=15\n",
    "init_gest = np.sort(np.random.choice(range(data_sh[2]),n,replace=False))\n",
    "print('Gestures randomly selected for initial training')\n",
    "print(init_gest)\n",
    "init_train_set = train_set[:,init_gest]\n",
    "init_val_set =  val_set[:,init_gest]\n",
    "init_test_set =  test_set[:,init_gest]\n",
    "init_ts_sh=np.shape(init_test_set)\n",
    "init_val_sh=np.shape(init_val_set)\n",
    "init_tr_sh=np.shape(init_train_set)\n",
    "\n",
    "def product(list):\n",
    "    p =1\n",
    "    for i in list:\n",
    "        p *= i\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "init_train_set=np.reshape(init_train_set,(product(init_tr_sh[:-2]),init_tr_sh[-2],init_tr_sh[-1]))\n",
    "init_val_set=np.reshape(init_val_set,(product(init_val_sh[:-2]),init_val_sh[-2],init_val_sh[-1]))\n",
    "init_test_set=np.reshape(init_test_set,(product(init_ts_sh[:-2]),init_ts_sh[-2],init_ts_sh[-1]))\n",
    "\n",
    "#check for zero array samples and remove\n",
    "init_train_set= init_train_set[np.nonzero(np.sum(init_train_set[:,:-1],axis=(1,2)))[0]]\n",
    "init_val_set= init_val_set[np.nonzero(np.sum(init_val_set[:,:-1],axis=(1,2)))[0]]\n",
    "init_test_set= init_test_set[np.nonzero(np.sum(init_test_set[:,:-1],axis=(1,2)))[0]]\n",
    "\n",
    "np.random.shuffle(init_train_set)\n",
    "np.random.shuffle(init_val_set)\n",
    "np.random.shuffle(init_test_set)\n",
    "\n",
    "ts_sh=np.shape(test_set)\n",
    "tr_sh=np.shape(train_set)\n",
    "val_sh=np.shape(val_set)\n",
    "train_set=np.reshape(train_set,(product(tr_sh[:-2]),tr_sh[-2],tr_sh[-1]))\n",
    "val_set=np.reshape(val_set,(product(val_sh[:-2]),val_sh[-2],val_sh[-1]))\n",
    "test_set=np.reshape(test_set,(product(ts_sh[:-2]),ts_sh[-2],ts_sh[-1]))\n",
    "train_set = train_set[np.nonzero(np.sum(train_set[:,:-1],axis=(1,2)))[0]]\n",
    "val_set = train_set[np.nonzero(np.sum(val_set[:,:-1],axis=(1,2)))[0]]\n",
    "test_set = test_set[np.nonzero(np.sum(test_set[:,:-1],axis=(1,2)))[0]]\n",
    "np.random.shuffle(train_set)\n",
    "np.random.shuffle(test_set)\n",
    "\n",
    "\n",
    "init_train_set_x = init_train_set[:,:-1]\n",
    "init_val_set_x = init_val_set[:,:-1]\n",
    "init_test_set_x = init_test_set[:,:-1]\n",
    "init_train_set_y = init_train_set[:,-1,1]\n",
    "init_val_set_y = init_val_set[:,-1,1]\n",
    "init_test_set_y = init_test_set[:,-1,1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% split up gestures for transfer learning\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gestures randomly selected for initial training\n",
      "[ 4  5  6  7  8  9 10 11 12 13 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Convolution2D, MaxPool2D, Flatten, Input, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#import system_query as sq\n",
    "from tensorflow.keras.models import Model\n",
    "from ttictoc import Timer\n",
    "\n",
    "####Neural network nlayer\n",
    "def network_2d(X_train,Y_train,layers=4):\n",
    "    sh_in = np.shape(X_train)\n",
    "    sh_out = np.shape(Y_train)\n",
    "    inputs_cnn = Input(shape=(sh_in[1],sh_in[2],1), name='inputs_cnn')\n",
    "    conv1_n = Convolution2D(filters=128, kernel_size=(6,1), activation='relu', input_shape=(sh_in[1],sh_in[2],1))(inputs_cnn)\n",
    "    batch1_n = BatchNormalization()(conv1_n)\n",
    "    pool1_n = MaxPool2D(pool_size=(3), strides=(2), padding=\"same\")(batch1_n)\n",
    "    for i in range(1,layers):\n",
    "        if i > 2:\n",
    "            f_S = 64\n",
    "        else:\n",
    "            f_S = 32\n",
    "        conv1_n = Convolution2D(f_S, (3,1), activation='relu')(pool1_n)\n",
    "        batch1_n = BatchNormalization()(conv1_n)\n",
    "        pool1_n = MaxPool2D(pool_size=(2), strides=(2), padding=\"same\")(batch1_n)\n",
    "    flatten = Flatten()(pool1_n)\n",
    "    dense_end1 = Dense(64, activation='relu')(flatten)\n",
    "    dense_end2 = Dense(32, activation='relu')(dense_end1)\n",
    "    main_output = Dense(sh_out[1], activation='softmax', name='main_output')(dense_end2)\n",
    "    model = Model(inputs=inputs_cnn, outputs=main_output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0005, decay=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_network_2d(model,X_train, y_train, X_test, y_test,n_train,b_size=1024,f_path=[],pt=False):\n",
    "    if f_path==[]:\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=100)]\n",
    "    #else:\n",
    "      #Qsys = sq.query_all()\n",
    "        #callbacks = [EarlyStopping(monitor='val_loss', patience=75),\n",
    "        #             ModelCheckpoint(filepath=f_path+Qsys['host'], monitor='val_loss', save_best_only=True)]\n",
    "    t = Timer()\n",
    "    t.start()\n",
    "    history = model.fit(X_train, y_train, epochs=n_train, callbacks=callbacks, batch_size=b_size,\n",
    "                    validation_data=(X_test, y_test))\n",
    "    elapsed = t.stop()\n",
    "    if pt:\n",
    "        print(n_train,'Epoch time:', elapsed)\n",
    "    return model, history\n",
    "init_U = np.unique(init_train_set_y)\n",
    "\n",
    "for i in range(len(init_U)):\n",
    "    init_train_set_y[init_train_set_y == init_U[i]]=i\n",
    "    init_val_set_y[init_val_set_y == init_U[i]] = i\n",
    "    init_test_set_y[init_test_set_y == init_U[i]] = i\n",
    "y_tr =tf.keras.utils.to_categorical(init_train_set_y, dtype='float32')\n",
    "y_v =tf.keras.utils.to_categorical(init_val_set_y, dtype='float32')\n",
    "y_ts =tf.keras.utils.to_categorical(init_test_set_y, dtype='float32')\n",
    "\n",
    "n_train = 1000\n",
    "model = network_2d(init_train_set_x,y_tr,4)\n",
    "\n",
    "train_network_2d(model,init_train_set_x,y_tr, init_val_set_x,y_v,n_train)\n",
    "print(\"Evaluate initial transfer model with all layers trainable on less classes on test data\")\n",
    "results = model.evaluate(init_test_set_x, y_ts, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "print('Initial model trained now popping final layer and replacing with full class')\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "train_set_x = train_set[:,:-1]\n",
    "val_set_x = val_set[:,:-1]\n",
    "test_set_x = test_set[:,:-1]\n",
    "train_set_y = train_set[:,-1,1]\n",
    "val_set_y = val_set[:,-1,1]\n",
    "test_set_y = test_set[:,-1,1]\n",
    "y_train =tf.keras.utils.to_categorical(train_set_y, dtype='float32')\n",
    "y_val =tf.keras.utils.to_categorical(val_set_y, dtype='float32')\n",
    "y_test =tf.keras.utils.to_categorical(test_set_y, dtype='float32')\n",
    "sh_out = np.shape(y_test)\n",
    "f_model = Sequential()\n",
    "f_model.add(model)\n",
    "f_model.layers.pop()\n",
    "for layer in f_model.layers:\n",
    "         layer.trainable=False\n",
    "f_model.add(Dense(sh_out[1], activation='softmax', name='main_output'))\n",
    "f_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0005, decay=1e-5), loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "f_model.summary()\n",
    "ft_model = f_model.copy()\n",
    "n_train = 500\n",
    "train_network_2d(f_model,train_set_x,y_train, val_set_x,y_val,n_train)\n",
    "print(\"Evaluate transfer model after training with only final layer trainable on test data\")\n",
    "results = f_model.evaluate(test_set_x, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "train_network_2d(ft_model,train_set_x,y_train, val_set_x,y_val,n_train)\n",
    "print(\"Evaluate transfer model after training with layer trainable on test data\")\n",
    "results = ft_model.evaluate(test_set_x, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "for layer in f_model.layers:\n",
    "         layer.trainable=True\n",
    "\n",
    "n_train = 100\n",
    "train_network_2d(f_model,train_set_x,y_train, val_set_x,y_val,n_train)\n",
    "print(\"Evaluate transfer model after more training with all layers trainable on test data\")\n",
    "results = f_model.evaluate(test_set_x, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)\n",
    "\n",
    "%##\n",
    "model_naive = network_2d(train_set_x,y_train,4)\n",
    "n_train = 500\n",
    "train_network_2d(model_naive,train_set_x,y_train, val_set_x,y_val,n_train)\n",
    "print(\"Evaluate naive model on test data\")\n",
    "results = model_naive.evaluate(test_set_x, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-bcb5cc46",
   "language": "python",
   "display_name": "PyCharm (medical)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}